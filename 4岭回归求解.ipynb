{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "岭回归（Ridge Regression）是一种线性回归的扩展方法，它在损失函数中引入了正则化项，以防止模型过拟合。以下是岭回归的目标函数、求解公式和迭代公式。\n",
    "\n",
    "1. 目标函数 (Objective Function)\n",
    "岭回归的目标是最小化以下损失函数：\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2 $$\n",
    "其中：\n",
    "$ h_\\theta(x) $：假设函数，即预测值；\n",
    "$ m $：样本数量；\n",
    "$ n $：特征数量（不包括偏置项）；\n",
    "$ \\lambda $：正则化参数，控制惩罚强度（$\\lambda > 0$）；\n",
    "$ \\theta_j $：模型参数（权重）；\n",
    "$ x^{(i)} $：第 $ i $ 个样本的输入特征；\n",
    "$ y^{(i)} $：第 $ i $ 个样本的真实值。\n",
    "假设函数形式\n",
    "对于多元线性回归，假设函数为：\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n = \\sum_{j=0}^{n} \\theta_j x_j $$\n",
    "其中 $ x_0 = 1 $，作为偏置项。\n",
    "\n",
    "2. 解析解公式 (Closed-form Solution)\n",
    "岭回归可以通过解析法直接求得最优参数 $ \\theta $。其解析解公式如下：\n",
    "$$ \\theta = (X^T X + \\lambda I)^{-1} X^T y $$\n",
    "其中：\n",
    "$ X $：包含所有样本特征的矩阵（维度为 $ m \\times (n+1) $），其中第一列为全 1（偏置项）；\n",
    "$ y $：目标值的列向量（维度为 $ m \\times 1 $）；\n",
    "$ I $：单位矩阵（维度为 $ (n+1) \\times (n+1) $）；\n",
    "$ \\lambda $：正则化参数。\n",
    "\n",
    "3. 梯度下降更新规则 (Gradient Descent Update Rule)\n",
    "如果使用梯度下降优化岭回归，则需要对目标函数进行求导并更新参数。\n",
    "损失函数\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2 $$\n",
    "参数更新规则\n",
    "对每个参数 $ \\theta_j $ 的更新规则如下：\n",
    "对于 $ j = 0 $（偏置项）： $$ \\theta_0 := \\theta_0 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} $$\n",
    "对于 $ j = 1, 2, ..., n $（非偏置项）： $$ \\theta_j := \\theta_j \\left(1 - \\alpha \\cdot \\frac{\\lambda}{m}\\right) - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} $$\n",
    "其中：\n",
    "$ \\alpha $：学习率；\n",
    "$ x_j^{(i)} $：第 $ i $ 个样本的第 $ j $ 个特征；\n",
    "$ h_\\theta(x^{(i)}) $：第 $ i $ 个样本的预测值；\n",
    "$ y^{(i)} $：第 $ i $ 个样本的真实值。\n",
    "\n",
    "4. 向量化形式 (Vectorized Form)\n",
    "在实际代码实现中，我们通常使用向量化计算以提高效率。假设：\n",
    "$ X $ 是输入特征矩阵（维度为 $ m \\times (n+1) $）；\n",
    "$ y $ 是目标值的列向量（维度为 $ m \\times 1 $）；\n",
    "$ \\theta $ 是参数的列向量（维度为 $ (n+1) \\times 1 $）；\n",
    "则梯度下降的更新规则可以写成：\n",
    "$$ \\theta := \\theta - \\alpha \\cdot \\left( \\frac{1}{m} X^T (X\\theta - y) + \\frac{\\lambda}{m} \\theta \\right) $$\n",
    "其中：\n",
    "$ X^T (X\\theta - y) $：均方误差部分的梯度；\n",
    "$ \\frac{\\lambda}{m} \\theta $：正则化项的梯度；\n",
    "$ \\alpha $：学习率。\n",
    "\n",
    "5. 示例代码（Python）"
   ],
   "id": "1200856b758c49a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def ridge_regression_gradient_descent(X, y, theta, learning_rate=0.01, num_iterations=1000, lambda_=1.0):\n",
    "    m = X.shape[0]  # 样本数量\n",
    "    costs = np.zeros(num_iterations)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (X.T @ errors) / m + (lambda_ / m) * theta\n",
    "        gradient[0] -= (lambda_ / m) * theta[0]  # 偏置项不参与正则化\n",
    "        theta -= learning_rate * gradient\n",
    "        costs[i] = np.sum(errors**2) / (2*m) + (lambda_ / (2*m)) * np.sum(theta[1:]**2)\n",
    "\n",
    "    return theta, costs\n"
   ],
   "id": "84072485a892f1b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
