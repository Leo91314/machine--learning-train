{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " # Boosting 详解\n",
    "\n",
    "**Boosting**是集成学习（Ensemble Learning）的一种重要方法。它通过**串行训练多个弱学习器**，每个新模型都着重修正前一个模型的错误，最终组合所有弱学习器的预测，形成一个强学习器。\n",
    "\n",
    "Boosting常用于**分类、回归和排序**任务。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、核心思想\n",
    "\n",
    "Boosting的目标是：\n",
    "- 将多个性能略高于随机猜测（弱学习器）的模型**集成为一个强模型**\n",
    "- 每轮训练时，**提高对前一轮分类错误样本的关注**\n",
    "- 通过加权投票或加权平均组合模型\n",
    "\n",
    "---\n",
    "\n",
    "## 二、与Bagging的区别\n",
    "\n",
    "| 特性             | Bagging                     | Boosting                 |\n",
    "|------------------|-----------------------------|--------------------------|\n",
    "| 学习器训练方式   | 并行独立                   | 串行依赖                 |\n",
    "| 样本权重调整     | 无（Bootstrap重采样）       | 每轮动态调整样本权重     |\n",
    "| 偏差与方差       | 降低方差                   | 降低偏差和方差           |\n",
    "| 基学习器         | 通常同质模型（如树）        | 弱学习器（如树桩）       |\n",
    "\n",
    "---\n",
    "\n",
    "## 三、Boosting的典型算法\n",
    "\n",
    "### 1. AdaBoost（Adaptive Boosting）\n",
    "\n",
    "最早的Boosting算法，主要特点：\n",
    "- 使用简单分类器（弱学习器），如决策树桩（单层树）\n",
    "- 每轮训练时调整样本权重：分类错的样本权重提升\n",
    "- 最终模型是加权投票\n",
    "\n",
    "**基本流程：**\n",
    "1. 初始化每个样本权重相同\n",
    "2. 重复迭代：\n",
    "   - 训练弱学习器\n",
    "   - 计算误差率 $\\epsilon$\n",
    "   - 计算弱学习器权重 $\\alpha$\n",
    "     $$\n",
    "     \\alpha = \\frac{1}{2}\\ln\\left(\\frac{1 - \\epsilon}{\\epsilon}\\right)\n",
    "     $$\n",
    "   - 更新样本权重\n",
    "3. 输出加权投票分类器\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Gradient Boosting\n",
    "\n",
    "利用**梯度下降思想**逐步优化损失函数：\n",
    "- 每轮训练一个新模型，拟合前一轮的残差（负梯度）\n",
    "- 最终模型是所有弱学习器的加权和\n",
    "\n",
    "**关键优势：**\n",
    "- 可灵活选择损失函数（平方误差、对数损失等）\n",
    "- 更强的拟合能力\n",
    "\n",
    "---\n",
    "\n",
    "### 3. XGBoost / LightGBM / CatBoost\n",
    "\n",
    "**Gradient Boosting的高效实现：**\n",
    "- **XGBoost**：支持正则化、分布式计算\n",
    "- **LightGBM**：基于直方图的高效分裂\n",
    "- **CatBoost**：对类别特征友好\n",
    "\n",
    "这些框架在比赛和工业应用中广泛使用。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、优缺点\n",
    "\n",
    "✅ **优点：**\n",
    "- 偏差低，泛化能力强\n",
    "- 能提升弱学习器性能\n",
    "- 可灵活适配不同损失函数\n",
    "\n",
    "❌ **缺点：**\n",
    "- 训练串行，速度慢（相较Bagging）\n",
    "- 对噪声敏感，易过拟合（需调参）\n",
    "- 模型解释性差\n",
    "\n",
    "---\n",
    "\n",
    "## 五、Python示例：AdaBoost\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 生成数据\n",
    "X, y = make_classification(n_samples=500, n_features=10, n_informative=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# 定义基学习器（决策树桩）\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# AdaBoost\n",
    "ada = AdaBoostClassifier(\n",
    "    base_estimator=base_estimator,\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 六、Python示例：Gradient Boosting\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 七、调参要点\n",
    "\n",
    "1. **n_estimators**：弱学习器数量\n",
    "   - 越多模型越复杂\n",
    "2. **learning_rate**：每个弱学习器的贡献\n",
    "   - 越小越稳健，需搭配较大n_estimators\n",
    "3. **max_depth**：树的深度\n",
    "   - 控制模型复杂度\n",
    "\n",
    "> **Tip:** 调参时一般先固定n_estimators，再调learning_rate和max_depth。\n",
    "\n",
    "---\n",
    "\n",
    "## 八、总结\n",
    "\n",
    "Boosting是集成学习的强大工具：\n",
    "- 串行地优化弱学习器\n",
    "- 能大幅降低偏差\n",
    "- 常用于工业竞赛和生产环境\n",
    "\n",
    "最常用Boosting框架：\n",
    "- AdaBoost（简单高效）\n",
    "- Gradient Boosting（灵活强大）\n",
    "- XGBoost / LightGBM / CatBoost（高效进阶）\n",
    "\n",
    "---"
   ],
   "id": "8f419a7c8e2c7594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "444a413028c2839e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
