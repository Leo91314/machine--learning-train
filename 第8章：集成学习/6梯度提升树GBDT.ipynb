{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 梯度提升树（GBDT）详解\n",
    "\n",
    "**梯度提升树（Gradient Boosting Decision Tree, GBDT）**是一种强大的集成学习方法，通过**将多个决策树顺序叠加**来不断减小预测误差。\n",
    "\n",
    "它在机器学习竞赛、工业应用中都有极其广泛的使用。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、核心思想\n",
    "\n",
    "- GBDT是**Boosting + 决策树**。\n",
    "- 每一轮训练一个新树，拟合上轮模型的残差（负梯度）。\n",
    "- 所有树的预测结果累加形成最终输出。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、算法流程\n",
    "\n",
    "假设目标是最小化一个损失函数：\n",
    "$$\n",
    "L = \\sum_{i=1}^n \\ell(y_i, F(x_i))\n",
    "$$\n",
    "\n",
    "**步骤：**\n",
    "\n",
    "1. 初始化模型：\n",
    "   $$\n",
    "   F_0(x) = \\arg\\min_\\gamma \\sum_i \\ell(y_i, \\gamma)\n",
    "   $$\n",
    "   - 回归：通常是样本均值。\n",
    "\n",
    "2. 对每一轮 $m=1,2,...,M$：\n",
    "   - 计算负梯度（残差）：\n",
    "     $$\n",
    "     r_i^{(m)} = -\\left[\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)}\n",
    "     $$\n",
    "   - 在$(x_i, r_i^{(m)})$上训练一棵回归树$h_m(x)$。\n",
    "   - 计算最佳步长：\n",
    "     $$\n",
    "     \\gamma_m = \\arg\\min_\\gamma \\sum_i \\ell(y_i, F_{m-1}(x_i) + \\gamma \\cdot h_m(x_i))\n",
    "     $$\n",
    "   - 更新模型：\n",
    "     $$\n",
    "     F_m(x) = F_{m-1}(x) + \\nu \\cdot \\gamma_m \\cdot h_m(x)\n",
    "     $$\n",
    "     其中$\\nu$是学习率（常取0.01~0.1）。\n",
    "\n",
    "3. 最终模型：\n",
    "   $$\n",
    "   F_M(x) = F_0(x) + \\sum_{m=1}^M \\nu \\cdot \\gamma_m \\cdot h_m(x)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## 三、支持的损失函数\n",
    "\n",
    "- 回归：平方误差、绝对误差\n",
    "- 二分类：对数损失（Logistic Loss）\n",
    "- 多分类：Softmax\n",
    "\n",
    "---\n",
    "\n",
    "## 四、GBDT与随机森林对比\n",
    "\n",
    "| 特性             | 随机森林                  | GBDT                     |\n",
    "|------------------|---------------------------|--------------------------|\n",
    "| 训练方式         | 并行独立训练多棵树        | 串行逐步拟合残差        |\n",
    "| 偏差/方差        | 降低方差                  | 同时降低偏差和方差      |\n",
    "| 样本重采样       | Bootstrap采样             | 可选（subsample）        |\n",
    "| 性能             | 稳定，偏差高              | 更强，易过拟合           |\n",
    "| 可解释性         | 中等                      | 较差                     |\n",
    "\n",
    "---\n",
    "\n",
    "## 五、优缺点\n",
    "\n",
    "✅ **优点**\n",
    "- 强大的预测性能\n",
    "- 支持多种损失函数\n",
    "- 可评估特征重要性\n",
    "- 调参灵活\n",
    "\n",
    "❌ **缺点**\n",
    "- 串行训练，速度慢\n",
    "- 对噪声敏感\n",
    "- 超参数多，需小心调参\n",
    "\n",
    "---\n",
    "\n",
    "## 六、Python示例（scikit-learn GBDT）\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 生成数据\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# 定义GBDT模型\n",
    "gbdt = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbdt.fit(X_train, y_train)\n",
    "y_pred = gbdt.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 七、调参要点\n",
    "\n",
    "1. **n_estimators**\n",
    "   - 弱学习器数量\n",
    "   - 越大模型越复杂\n",
    "2. **learning_rate**\n",
    "   - 学习率（步长）\n",
    "   - 越小越稳健，需配合增大n_estimators\n",
    "3. **max_depth**\n",
    "   - 单棵树深度\n",
    "   - 控制复杂度\n",
    "4. **subsample**\n",
    "   - 子样本比例\n",
    "   - <1.0时相当于随机抽样，可防过拟合\n",
    "5. **min_samples_leaf**\n",
    "   - 每个叶节点最少样本数\n",
    "\n",
    "> 一般先调`n_estimators`和`learning_rate`，再调`max_depth`。\n",
    "\n",
    "---\n",
    "\n",
    "## 八、进阶：XGBoost / LightGBM / CatBoost\n",
    "\n",
    "GBDT的高性能实现：\n",
    "- **XGBoost**\n",
    "  - 支持正则化\n",
    "  - 并行高效\n",
    "- **LightGBM**\n",
    "  - 直方图优化\n",
    "  - 更快\n",
    "- **CatBoost**\n",
    "  - 对类别特征支持好\n",
    "\n",
    "---\n",
    "\n",
    "## 九、总结\n",
    "\n",
    "**GBDT**是一种强大的提升树方法：\n",
    "- 逐步拟合残差，提升拟合能力\n",
    "- 广泛用于回归与分类\n",
    "- 推荐使用XGBoost / LightGBM / CatBoost在生产中\n",
    "\n",
    "---"
   ],
   "id": "9c638c6164e64f7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
