{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 逻辑回归简介\n",
    "逻辑回归（Logistic Regression）是一种二分类算法，用于预测样本的类别。\n",
    "1. 假设函数（Hypothesis Function）\n",
    "逻辑回归通过线性组合输入特征 $ x_1, x_2, ..., x_n $，并将其映射到一个概率值（0到1之间），表示属于某一类的概率。\n",
    "假设函数定义如下：\n",
    "$$ h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}} $$\n",
    "其中：\n",
    "$ \\theta $ 是参数向量。\n",
    "$ x $ 是输入特征向量（包括常数项 $x_0=1$）。\n",
    "$ \\theta^T x $ 表示线性组合 $ \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_n x_n $。\n",
    "$ h_\\theta(x) $ 输出的是样本 $x$ 属于正类（类别为1）的概率，即 $ P(y=1|x;\\theta) $。\n",
    "该函数也被称为 Sigmoid 函数 或 Logistic 函数，其输出范围是 (0, 1)，形状类似于 \"S\" 曲线。\n",
    "\n",
    "2. 成本函数（Cost Function）\n",
    "为了评估和优化模型参数 $ \\theta $，我们需要一个成本函数来衡量预测结果与真实标签之间的差异。\n",
    "逻辑回归的成本函数定义如下：\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))] $$\n",
    "其中：\n",
    "$ m $ 是训练样本的数量。\n",
    "$ y^{(i)} $ 是第 $ i $ 个样本的真实标签（取值为 0 或 1）。\n",
    "$ h_\\theta(x^{(i)}) $ 是第 $ i $ 个样本的预测概率。\n",
    "直观理解：\n",
    "当 $ y=1 $ 时，如果预测值越接近 1，成本越低；反之，预测值越接近 0，成本急剧上升。\n",
    "当 $ y=0 $ 时，如果预测值越接近 0，成本越低；反之，预测值越接近 1，成本急剧上升。\n",
    "\n",
    "总结\n",
    "逻辑回归的核心思想是通过 Sigmoid 函数将线性回归的结果映射到 [0,1] 区间，从而表示样本属于某一类别的概率。然后使用成本函数量化误差，并通过梯度下降等优化算法找到最优参数 $ \\theta $，使得预测结果尽可能准确。"
   ],
   "id": "997c1eb165c77484"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 损失函数与代价函数\n",
    "在机器学习中，损失函数（Loss Function） 和 代价函数（Cost Function） 是密切相关的概念，它们用于衡量模型的预测结果与真实值之间的差异。虽然这两个术语有时会被混用，但它们之间有细微的区别：\n",
    "\n",
    "1. 损失函数（Loss Function）\n",
    "损失函数用于衡量单个样本的预测误差。\n",
    "它是关于模型输出和单个样本真实标签的函数。\n",
    "示例：逻辑回归的损失函数\n",
    "对于逻辑回归中的一个样本 $ (x^{(i)}, y^{(i)}) $，其损失函数定义如下： $$ L(h_\\theta(x^{(i)}), y^{(i)}) = -y^{(i)} \\log(h_\\theta(x^{(i)})) - (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)})) $$\n",
    "其中：\n",
    "$ h_\\theta(x^{(i)}) $ 是模型对第 $ i $ 个样本的预测概率。\n",
    "$ y^{(i)} $ 是第 $ i $ 个样本的真实标签（0 或 1）。\n",
    "这个函数会根据预测值与真实值的差距计算出一个损失值：\n",
    "当预测正确时，损失较小；\n",
    "当预测错误时，损失较大。\n",
    "\n",
    "2. 代价函数（Cost Function）\n",
    "代价函数是整个训练集上所有样本损失的平均值，用于衡量模型在整个数据集上的整体误差。\n",
    "它是损失函数的扩展，考虑了所有训练样本。\n",
    "示例：逻辑回归的代价函数\n",
    "对于逻辑回归，代价函数定义为所有样本损失的平均值： $$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} L(h_\\theta(x^{(i)}), y^{(i)}) $$ 将具体的损失函数代入后，可以得到： $$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))] $$\n",
    "其中：\n",
    "$ m $ 是训练样本的数量。\n",
    "$ y^{(i)} $ 是第 $ i $ 个样本的真实标签。\n",
    "$ h_\\theta(x^{(i)}) $ 是第 $ i $ 个样本的预测概率。\n",
    "\n",
    "3. 总结\n",
    "损失函数关注的是单个样本的预测误差。\n",
    "代价函数是整个训练集上损失函数的平均值，用于优化模型参数。\n",
    "在逻辑回归中，损失函数的设计基于最大似然估计的思想，目的是最大化正确分类的概率。\n",
    "通过最小化代价函数，我们可以找到最优的模型参数 $ \\theta $，从而提高模型的预测准确性。"
   ],
   "id": "8426eeab19771206"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
