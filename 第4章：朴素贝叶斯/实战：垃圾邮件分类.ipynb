{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 垃圾邮件识别\n",
    "文本处理：\n",
    "1. 词法分析：将文本切分为单词\n",
    "2. 词向量化：将单词映射为向量\n",
    "3. 模型训练：使用向量训练模型\n",
    "4. 模型预测：使用模型对未知文本进行预测"
   ],
   "id": "509ac1c4f4983815"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.108484Z",
     "start_time": "2025-06-27T12:28:49.105891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import naive_bayes as nb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy import io"
   ],
   "id": "f21a9c6b3e695091",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.118017Z",
     "start_time": "2025-06-27T12:28:49.115759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    vocabList = list(vocabSet)\n",
    "    vocabList = sorted(vocabList)\n",
    "    return vocabList"
   ],
   "id": "86923e37b4e72de",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试",
   "id": "95ca5d426ba3e3c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.127916Z",
     "start_time": "2025-06-27T12:28:49.125140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataSet = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "           ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "           ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "           ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "           ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "           ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "vocabList = createVocabList(dataSet)\n",
    "print(vocabList)"
   ],
   "id": "c62a4784656c4aa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ate', 'buying', 'cute', 'dalmation', 'dog', 'flea', 'food', 'garbage', 'has', 'help', 'him', 'how', 'is', 'licks', 'love', 'maybe', 'mr', 'my', 'not', 'park', 'please', 'posting', 'problems', 'quit', 'so', 'steak', 'stop', 'stupid', 'take', 'to', 'worthless']\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "词集模型\n",
    "1. 创建一个长度为单词数量，值为0的向量\n",
    "2. 遍历单词列表，将单词列表中的单词作为索引，将向量的对应位置的值设为1\n",
    "3. 返回向量"
   ],
   "id": "32e52934079697d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.139003Z",
     "start_time": "2025-06-27T12:28:49.136888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ],
   "id": "70a6156232c46ed1",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.146682Z",
     "start_time": "2025-06-27T12:28:49.144906Z"
    }
   },
   "cell_type": "code",
   "source": "print(setOfWords2Vec(vocabList, dataSet[0]))",
   "id": "aacf801e148989",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 词袋模型\n",
    "1. 创建一个长度为单词数量，值为0的向量\n",
    "2.\n"
   ],
   "id": "5748fe74e621936f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.156197Z",
     "start_time": "2025-06-27T12:28:49.153827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bag0forWords2Vec(vocabList, inputSet):  # 词袋模型函数\n",
    "    \"\"\"\n",
    "    将输入文本转换为词袋向量表示。\n",
    "\n",
    "    参数:\n",
    "    vocabList (list): 已构建的词汇表列表\n",
    "    inputSet (set or list): 输入的文本内容（分词后的单词集合或列表）\n",
    "\n",
    "    返回:\n",
    "    list: 与词汇表长度一致的向量，表示输入文本中每个词的出现次数\n",
    "    \"\"\"\n",
    "    # 初始化一个全为0的向量，长度等于词汇表长度\n",
    "    returnVec = [0] * len(vocabList)\n",
    "\n",
    "    # 遍历输入文本中的每个单词\n",
    "    for word in inputSet:\n",
    "        # 如果单词在词汇表中存在\n",
    "        if word in vocabList:\n",
    "            # 在对应位置增加计数（这里是简单+1，即词袋模型）\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        else:\n",
    "            # 如果单词不在词汇表中，打印警告信息\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "\n",
    "    # 返回最终的词袋向量表示\n",
    "    return returnVec\n"
   ],
   "id": "6081a57480230d14",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.164717Z",
     "start_time": "2025-06-27T12:28:49.162655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(vocabList)\n",
    "print(bag0forWords2Vec(vocabList, dataSet[0]))"
   ],
   "id": "c98dd237f3f6ad67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ate', 'buying', 'cute', 'dalmation', 'dog', 'flea', 'food', 'garbage', 'has', 'help', 'him', 'how', 'is', 'licks', 'love', 'maybe', 'mr', 'my', 'not', 'park', 'please', 'posting', 'problems', 'quit', 'so', 'steak', 'stop', 'stupid', 'take', 'to', 'worthless']\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:32:43.321952Z",
     "start_time": "2025-06-27T12:32:43.315686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "textParse('I love You')"
   ],
   "id": "9768b46064678fc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.179452Z",
     "start_time": "2025-06-27T12:28:49.177142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loaddata():\n",
    "    docList = []\n",
    "    classList = []\n",
    "\n",
    "    num = 26\n",
    "    for i in range(1, num):\n",
    "        wordList = textParse(open('data/email/ham/%d.txt' % i, encoding='latin-1').read())\n",
    "\n",
    "        docList.append(wordList)\n",
    "        classList.append(1)\n",
    "\n",
    "        wordList = textParse(open('data/email/spam/%d.txt' % i, encoding='latin-1').read())\n",
    "\n",
    "        docList.append(wordList)\n",
    "        classList.append(0)\n",
    "\n",
    "    vocabList = createVocabList(docList)\n",
    "\n",
    "    X = []\n",
    "    for docIndex in range(len(docList)):\n",
    "        X.append(bag0forWords2Vec(vocabList, docList[docIndex]))\n",
    "        # X.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "\n",
    "    return X, classList, vocabList"
   ],
   "id": "1ff6f3a8481faa37",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.189616Z",
     "start_time": "2025-06-27T12:28:49.185316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X,y,vocaList = loaddata()\n",
    "print(len(X), len(y))\n",
    "print(len(vocaList))"
   ],
   "id": "a48fb4e7018b5852",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n",
      "0\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:28:49.231491Z",
     "start_time": "2025-06-27T12:28:49.202724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = nb.MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "id": "539a9f5d155a0d39",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(40, 0)) while a minimum of 1 is required by MultinomialNB.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m nb\u001B[38;5;241m.\u001B[39mMultinomialNB()\n\u001B[0;32m----> 3\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(accuracy_score(y_test, y_pred))\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/pytorch2023/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/pytorch2023/lib/python3.9/site-packages/sklearn/naive_bayes.py:735\u001B[0m, in \u001B[0;36m_BaseDiscreteNB.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    714\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    715\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    716\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001B[39;00m\n\u001B[1;32m    717\u001B[0m \n\u001B[1;32m    718\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    733\u001B[0m \u001B[38;5;124;03m        Returns the instance itself.\u001B[39;00m\n\u001B[1;32m    734\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 735\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    736\u001B[0m     _, n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    738\u001B[0m     labelbin \u001B[38;5;241m=\u001B[39m LabelBinarizer()\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/pytorch2023/lib/python3.9/site-packages/sklearn/naive_bayes.py:581\u001B[0m, in \u001B[0;36m_BaseDiscreteNB._check_X_y\u001B[0;34m(self, X, y, reset)\u001B[0m\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_check_X_y\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    580\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 581\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mvalidate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/pytorch2023/lib/python3.9/site-packages/sklearn/utils/validation.py:2961\u001B[0m, in \u001B[0;36mvalidate_data\u001B[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[0m\n\u001B[1;32m   2959\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[1;32m   2960\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2961\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2962\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[1;32m   2964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/pytorch2023/lib/python3.9/site-packages/sklearn/utils/validation.py:1370\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m   1364\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1365\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1366\u001B[0m     )\n\u001B[1;32m   1368\u001B[0m ensure_all_finite \u001B[38;5;241m=\u001B[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001B[0;32m-> 1370\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1371\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1372\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1373\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_large_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1374\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1375\u001B[0m \u001B[43m    \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1376\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1377\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_writeable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1378\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1379\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1380\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_nd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1381\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_min_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1382\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_min_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_min_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1383\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1384\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1385\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1387\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[1;32m   1389\u001B[0m check_consistent_length(X, y)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/pytorch2023/lib/python3.9/site-packages/sklearn/utils/validation.py:1139\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m   1137\u001B[0m     n_features \u001B[38;5;241m=\u001B[39m array\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   1138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_features \u001B[38;5;241m<\u001B[39m ensure_min_features:\n\u001B[0;32m-> 1139\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1140\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m feature(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1141\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m a minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1142\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_features, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_features, context)\n\u001B[1;32m   1143\u001B[0m         )\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_non_negative:\n\u001B[1;32m   1146\u001B[0m     whom \u001B[38;5;241m=\u001B[39m input_name\n",
      "\u001B[0;31mValueError\u001B[0m: Found array with 0 feature(s) (shape=(40, 0)) while a minimum of 1 is required by MultinomialNB."
     ]
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
